{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.22/00\n"
     ]
    }
   ],
   "source": [
    "from makedataset import makedataset\n",
    "from numpy.testing._private.utils import HAS_LAPACK64\n",
    "from tensorflow.python.keras.saving.save import load_model\n",
    "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from root_numpy import array2root\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['geninvis_px', 'geninvis_py', 'geninvis_pz', 'geninvis_e', 'genmuon_px', 'genmuon_py', 'genmuon_pz', 'genmuon_e', 'muon_px', 'muon_py', 'muon_pz', 'muon_e', 'invis_px', 'invis_py', 'boson_M']\n",
      "---------------  dimensions of the input variables  ---------------\n",
      "training   input dimensions (10000, 8)    (10000,)\n",
      "predicitng input dimensions (10000, 6)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "data = makedataset('W2LNu10000Events_13Tev.root')\n",
    "\n",
    "X_train = data.getGenFeatureVals()\n",
    "y_train = data.getGenLabelVals()\n",
    "X_star  = data.getRecoFeatureVals()\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "print('-'*15,' dimensions of the input variables ','-'*15)\n",
    "print('training   input dimensions',X_train.shape,'  ',y_train.shape)\n",
    "print('predicitng input dimensions',X_star.shape)\n",
    "print('-'*50)\n",
    "num_pipeline = Pipeline([\n",
    "    #('imputer',SimpleImputer(strategy=\"median\")),\n",
    "    ('std_scaler',StandardScaler()),\n",
    "])\n",
    "\n",
    "X_gen,X_reco = num_pipeline.fit_transform(X_train),num_pipeline.fit_transform(X_star)\n",
    "M_gen  = num_pipeline.fit_transform(y_train.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANreco:\n",
    "    def __init__(self,genData,recoData,label):\n",
    "        self.genData  = genData\n",
    "        self.recoData = recoData\n",
    "        self.label  = label\n",
    "        self.input_dim  = genData.shape[1]\n",
    "        self.latent_dim = genData.shape[1]\n",
    "        \n",
    "        self.cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self.generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "        self.discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "        \n",
    "        self.generator = self.build_generator(self.input_dim,self.latent_dim)\n",
    "        self.discriminator = self.build_discriminator(self.input_dim)\n",
    "\n",
    "        checkpoint_dir = './training_checkpoints'\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "        checkpoint = tf.train.Checkpoint(generator_optimizer=self.generator_optimizer,\n",
    "                                        discriminator_optimizer=self.discriminator_optimizer,\n",
    "                                         generator=self.generator,\n",
    "                                         discriminator=self.discriminator)\n",
    "\n",
    "    def combine(self,t1,t2):\n",
    "        return tf.concat([t1,t2],axis=1)\n",
    "    \n",
    "#     @tf.function\n",
    "#     def train_step(self):\n",
    "#         noise = tf.random.normal([BATCH_SIZE,self.input_dim])\n",
    "#         print(type(dataset))\n",
    "#         print(type(noise))\n",
    "#         with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "#             genInput = self.combine(noise,label_dataset)\n",
    "#             genvals = self.generator(genInput, training=True)\n",
    "\n",
    "#             gen_reco = tf.concat([self.recoData[n],genvals[:,-2],genvals[:,-1]],axis=1)\n",
    "#             real_output = self.discriminator([self.genData[n],self.label[n]], training=True)\n",
    "#             fake_output = self.discriminator([gen_reco,self.label[n]], training=True)\n",
    "\n",
    "#         gradients_of_generator = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "#         gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "\n",
    "#         generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "#         discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    def train(self, epochs):\n",
    "        BATCH_SIZE = 100\n",
    "        genData_tensor = tf.constant(X_gen,dtype=tf.float32)\n",
    "        recoData_tensor = tf.constant(X_reco,dtype=tf.float32)\n",
    "        label_tensor = tf.constant(M_gen,dtype=tf.float32)\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(genData_tensor).batch(BATCH_SIZE)\n",
    "        recodataset = tf.data.Dataset.from_tensor_slices(recoData_tensor).batch(BATCH_SIZE)\n",
    "        label_dataset = tf.data.Dataset.from_tensor_slices(label_tensor).batch(BATCH_SIZE)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for feature_batch, label_batch,reco_feature_batch in zip(dataset, label_dataset,recodataset):\n",
    "                if len(feature_batch) == BATCH_SIZE:\n",
    "                    noise = tf.random.normal([BATCH_SIZE, self.input_dim])\n",
    "                with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "                    noiseandlabels = self.combine(noise,label_batch)\n",
    "                    generated_kin = self.generator(noiseandlabels, training=True)\n",
    "                    real_batch  = self.combine(feature_batch,label_batch)\n",
    "                    fake_batch  = self.generate_fake_samples(generated_kin,reco_feature_batch)\n",
    "                    real_output = self.discriminator(real_batch, training=True)\n",
    "                    fake_output = self.discriminator(fake_batch, training=True)\n",
    "                    \n",
    "                    gen_loss = self.generator_loss(fake_output)\n",
    "                    disc_loss = self.discriminator_loss(real_output, fake_output)\n",
    "                \n",
    "                gradients_of_generator = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "                gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "                \n",
    "                generator_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n",
    "                discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n",
    "        \n",
    "\n",
    "    def generate_fake_samples(self,gen_sample,reco_sample):\n",
    "        new_te = tf.concat([reco_sample,tf.expand_dims(gen_sample[:,-3],axis=1),\n",
    "                            tf.expand_dims(gen_sample[:,-2],axis=1),tf.expand_dims(gen_sample[:,-1],axis=1)],axis=1)\n",
    "        return new_te\n",
    "        \n",
    "    def discriminator_loss(self,real_output, fake_output):\n",
    "        real_loss = self.cross_entropy(tf.ones_like(real_output), real_output)\n",
    "        fake_loss = self.cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "        total_loss = real_loss + fake_loss\n",
    "        return total_loss\n",
    "\n",
    "    def generator_loss(self,fake_output):\n",
    "        return self.cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "                                 \n",
    "        \n",
    "    def build_generator(self,input_dim,latent_dim):\n",
    "        gen_input = layers.Input(shape=(input_dim+1,))\n",
    "#         gen_ytrue = layers.Input(shape=(1,))\n",
    "#         gen_concat = layers.Concatenate()([gen_input,gen_ytrue])\n",
    "        layer = layers.Dense(64,activation='relu')(gen_input)\n",
    "        layer = layers.Dense(256,activation='relu')(layer)\n",
    "        layer = layers.Dense(64,activation='relu')(layer)\n",
    "        out_layer = layers.Dense(latent_dim,activation='tanh')(layer)\n",
    "\n",
    "        model = tf.keras.Model(gen_input,out_layer)\n",
    "        return model\n",
    "\n",
    "    def build_discriminator(self,n_inputs):\n",
    "        dis_input = layers.Input(shape=(n_inputs+1,))\n",
    "#         dis_ytrue = layers.Input(shape=(1,))\n",
    "#         dis_concat = layers.Concatenate()([dis_input,dis_ytrue])\n",
    "        layer = layers.Dense(128,activation='relu')(dis_input)\n",
    "        layer = layers.Dense(64,activation='relu')(layer)\n",
    "        layer = layers.Dense(32,activation='relu')(layer)\n",
    "        out_layer = layers.Dense(1,activation='sigmoid')(layer)\n",
    "        \n",
    "        model = tf.keras.Model(dis_input,out_layer)\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "reco_obj = GANreco(genData_tensor,X_reco,label_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "reco_obj.train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4805575 ,  0.08972308, -0.21913351, -0.42678538,  0.3603556 ,\n",
       "        -0.02669153,  0.20076744, -0.89231825],\n",
       "       [-0.9145086 ,  0.8320737 ,  3.6767359 ,  3.759279  ,  0.5156228 ,\n",
       "        -0.6371764 ,  1.2647642 ,  0.6899558 ]], dtype=float32)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix = np.random.randint(0,10000,32)\n",
    "X_gen[[5,10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
